===== Á¨¨‰∏ÄÊ≠•ÔºöÂêØÂä®Ê£ÄÁ¥¢ÊúçÂä° =====
ÂêØÂä®Ê£ÄÁ¥¢ÊúçÂä°ÔºåÊó•Âøó‰øùÂ≠òÂà∞ /public_hw/share/cit_ztyu/cz/Search-R1/logs/10665_retriever.log
Ê£ÄÁ¥¢ÊúçÂä°PID: 2578667
===== Á¨¨‰∫åÊ≠•ÔºöÁ≠âÂæÖÊ£ÄÁ¥¢ÊúçÂä°Â∞±Áª™ =====
Ê£ÄÁ¥¢ÊúçÂä°Â∞±Áª™ÔºÅ
===== Á¨¨‰∏âÊ≠•ÔºöÂêØÂä® PPO ËÆ≠ÁªÉ =====
ËÆ≠ÁªÉ‰∏ªÁ´ØÂè£: 41106
2026-02-08 09:43:35,898	INFO worker.py:2012 -- Started a local Ray instance.
/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
[36m(main_task pid=2593262)[0m {'actor_rollout_ref': {'actor': {'clip_ratio': 0.2,
[36m(main_task pid=2593262)[0m                                  'entropy_coeff': 0.001,
[36m(main_task pid=2593262)[0m                                  'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2593262)[0m                                                  'grad_offload': False,
[36m(main_task pid=2593262)[0m                                                  'optimizer_offload': False,
[36m(main_task pid=2593262)[0m                                                  'param_offload': False,
[36m(main_task pid=2593262)[0m                                                  'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2593262)[0m                                  'grad_clip': 1.0,
[36m(main_task pid=2593262)[0m                                  'kl_loss_coef': 0.001,
[36m(main_task pid=2593262)[0m                                  'kl_loss_type': 'low_var_kl',
[36m(main_task pid=2593262)[0m                                  'optim': {'lr': 1e-06,
[36m(main_task pid=2593262)[0m                                            'lr_warmup_steps_ratio': 0.285,
[36m(main_task pid=2593262)[0m                                            'min_lr_ratio': None,
[36m(main_task pid=2593262)[0m                                            'total_training_steps': -1,
[36m(main_task pid=2593262)[0m                                            'warmup_style': 'constant'},
[36m(main_task pid=2593262)[0m                                  'ppo_epochs': 1,
[36m(main_task pid=2593262)[0m                                  'ppo_max_token_len_per_gpu': 16384,
[36m(main_task pid=2593262)[0m                                  'ppo_micro_batch_size': 16,
[36m(main_task pid=2593262)[0m                                  'ppo_mini_batch_size': 64,
[36m(main_task pid=2593262)[0m                                  'shuffle': False,
[36m(main_task pid=2593262)[0m                                  'state_masking': False,
[36m(main_task pid=2593262)[0m                                  'strategy': 'fsdp',
[36m(main_task pid=2593262)[0m                                  'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2593262)[0m                                  'use_dynamic_bsz': False,
[36m(main_task pid=2593262)[0m                                  'use_kl_loss': True},
[36m(main_task pid=2593262)[0m                        'hybrid_engine': True,
[36m(main_task pid=2593262)[0m                        'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2593262)[0m                                  'external_lib': None,
[36m(main_task pid=2593262)[0m                                  'override_config': {},
[36m(main_task pid=2593262)[0m                                  'path': '/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct',
[36m(main_task pid=2593262)[0m                                  'use_remove_padding': True},
[36m(main_task pid=2593262)[0m                        'ref': {'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2593262)[0m                                                'param_offload': False,
[36m(main_task pid=2593262)[0m                                                'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2593262)[0m                                'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=2593262)[0m                                'log_prob_micro_batch_size': 64,
[36m(main_task pid=2593262)[0m                                'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=2593262)[0m                                'ulysses_sequence_parallel_size': 1},
[36m(main_task pid=2593262)[0m                        'rollout': {'do_sample': True,
[36m(main_task pid=2593262)[0m                                    'dtype': 'bfloat16',
[36m(main_task pid=2593262)[0m                                    'enforce_eager': True,
[36m(main_task pid=2593262)[0m                                    'free_cache_engine': False,
[36m(main_task pid=2593262)[0m                                    'gpu_memory_utilization': 0.4,
[36m(main_task pid=2593262)[0m                                    'ignore_eos': False,
[36m(main_task pid=2593262)[0m                                    'load_format': 'dummy_dtensor',
[36m(main_task pid=2593262)[0m                                    'log_prob_max_token_len_per_gpu': 16384,
[36m(main_task pid=2593262)[0m                                    'log_prob_micro_batch_size': 64,
[36m(main_task pid=2593262)[0m                                    'log_prob_use_dynamic_bsz': False,
[36m(main_task pid=2593262)[0m                                    'max_num_batched_tokens': 8192,
[36m(main_task pid=2593262)[0m                                    'max_num_seqs': 1024,
[36m(main_task pid=2593262)[0m                                    'n': 1,
[36m(main_task pid=2593262)[0m                                    'n_agent': 1,
[36m(main_task pid=2593262)[0m                                    'name': 'vllm',
[36m(main_task pid=2593262)[0m                                    'prompt_length': 4096,
[36m(main_task pid=2593262)[0m                                    'response_length': 2048,
[36m(main_task pid=2593262)[0m                                    'temperature': 1.0,
[36m(main_task pid=2593262)[0m                                    'tensor_model_parallel_size': 1,
[36m(main_task pid=2593262)[0m                                    'top_k': -1,
[36m(main_task pid=2593262)[0m                                    'top_p': 0.95}},
[36m(main_task pid=2593262)[0m  'algorithm': {'adv_estimator': 'gae',
[36m(main_task pid=2593262)[0m                'gamma': 1.0,
[36m(main_task pid=2593262)[0m                'kl_ctrl': {'kl_coef': 0.001, 'type': 'fixed'},
[36m(main_task pid=2593262)[0m                'kl_penalty': 'kl',
[36m(main_task pid=2593262)[0m                'lam': 1.0,
[36m(main_task pid=2593262)[0m                'no_think_rl': False,
[36m(main_task pid=2593262)[0m                'state_masking': {'end_state_marker': '</information>',
[36m(main_task pid=2593262)[0m                                  'start_state_marker': '<information>'}},
[36m(main_task pid=2593262)[0m  'critic': {'cliprange_value': 0.5,
[36m(main_task pid=2593262)[0m             'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2593262)[0m             'forward_micro_batch_size': 16,
[36m(main_task pid=2593262)[0m             'grad_clip': 1.0,
[36m(main_task pid=2593262)[0m             'model': {'enable_gradient_checkpointing': True,
[36m(main_task pid=2593262)[0m                       'external_lib': None,
[36m(main_task pid=2593262)[0m                       'fsdp_config': {'fsdp_size': -1,
[36m(main_task pid=2593262)[0m                                       'grad_offload': False,
[36m(main_task pid=2593262)[0m                                       'optimizer_offload': False,
[36m(main_task pid=2593262)[0m                                       'param_offload': False,
[36m(main_task pid=2593262)[0m                                       'wrap_policy': {'min_num_params': 0}},
[36m(main_task pid=2593262)[0m                       'override_config': {},
[36m(main_task pid=2593262)[0m                       'path': '/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct',
[36m(main_task pid=2593262)[0m                       'tokenizer_path': '/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct',
[36m(main_task pid=2593262)[0m                       'use_remove_padding': True},
[36m(main_task pid=2593262)[0m             'optim': {'lr': 1e-05,
[36m(main_task pid=2593262)[0m                       'lr_warmup_steps_ratio': 0.015,
[36m(main_task pid=2593262)[0m                       'min_lr_ratio': None,
[36m(main_task pid=2593262)[0m                       'total_training_steps': -1,
[36m(main_task pid=2593262)[0m                       'warmup_style': 'constant'},
[36m(main_task pid=2593262)[0m             'ppo_epochs': 1,
[36m(main_task pid=2593262)[0m             'ppo_max_token_len_per_gpu': 32768,
[36m(main_task pid=2593262)[0m             'ppo_micro_batch_size': 16,
[36m(main_task pid=2593262)[0m             'ppo_mini_batch_size': 64,
[36m(main_task pid=2593262)[0m             'shuffle': False,
[36m(main_task pid=2593262)[0m             'strategy': 'fsdp',
[36m(main_task pid=2593262)[0m             'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2593262)[0m             'use_dynamic_bsz': False},
[36m(main_task pid=2593262)[0m  'data': {'max_obs_length': 2048,
[36m(main_task pid=2593262)[0m           'max_prompt_length': 4096,
[36m(main_task pid=2593262)[0m           'max_response_length': 2048,
[36m(main_task pid=2593262)[0m           'max_start_length': 2048,
[36m(main_task pid=2593262)[0m           'prompt_key': 'prompt',
[36m(main_task pid=2593262)[0m           'return_raw_chat': False,
[36m(main_task pid=2593262)[0m           'return_raw_input_ids': False,
[36m(main_task pid=2593262)[0m           'shuffle_train_dataloader': True,
[36m(main_task pid=2593262)[0m           'tokenizer': None,
[36m(main_task pid=2593262)[0m           'train_batch_size': 128,
[36m(main_task pid=2593262)[0m           'train_data_num': None,
[36m(main_task pid=2593262)[0m           'train_files': 'data/nq_search/train.parquet',
[36m(main_task pid=2593262)[0m           'val_batch_size': 128,
[36m(main_task pid=2593262)[0m           'val_data_num': 128,
[36m(main_task pid=2593262)[0m           'val_files': 'data/nq_search/test.parquet'},
[36m(main_task pid=2593262)[0m  'do_search': True,
[36m(main_task pid=2593262)[0m  'max_turns': 2,
[36m(main_task pid=2593262)[0m  'retriever': {'topk': 3, 'url': 'http://127.0.0.1:8009/retrieve'},
[36m(main_task pid=2593262)[0m  'reward_model': {'enable': False,
[36m(main_task pid=2593262)[0m                   'final_format_score': 0,
[36m(main_task pid=2593262)[0m                   'forward_max_token_len_per_gpu': 32768,
[36m(main_task pid=2593262)[0m                   'max_length': None,
[36m(main_task pid=2593262)[0m                   'micro_batch_size': 64,
[36m(main_task pid=2593262)[0m                   'model': {'external_lib': None,
[36m(main_task pid=2593262)[0m                             'fsdp_config': {'min_num_params': 0,
[36m(main_task pid=2593262)[0m                                             'param_offload': False},
[36m(main_task pid=2593262)[0m                             'input_tokenizer': '/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct',
[36m(main_task pid=2593262)[0m                             'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
[36m(main_task pid=2593262)[0m                             'use_remove_padding': False},
[36m(main_task pid=2593262)[0m                   'retrieval_score': 0,
[36m(main_task pid=2593262)[0m                   'strategy': 'fsdp',
[36m(main_task pid=2593262)[0m                   'structure_format_score': 0,
[36m(main_task pid=2593262)[0m                   'ulysses_sequence_parallel_size': 1,
[36m(main_task pid=2593262)[0m                   'use_dynamic_bsz': False},
[36m(main_task pid=2593262)[0m  'trainer': {'critic_warmup': 0,
[36m(main_task pid=2593262)[0m              'default_hdfs_dir': None,
[36m(main_task pid=2593262)[0m              'default_local_dir': 'verl_checkpoints/nq-search-r1-ppo-qwen2.5-3b-instruct-em',
[36m(main_task pid=2593262)[0m              'experiment_name': 'nq-search-r1-ppo-qwen2.5-3b-instruct-em',
[36m(main_task pid=2593262)[0m              'logger': ['wandb'],
[36m(main_task pid=2593262)[0m              'n_gpus_per_node': 2,
[36m(main_task pid=2593262)[0m              'nnodes': 1,
[36m(main_task pid=2593262)[0m              'project_name': 'Search-R1',
[36m(main_task pid=2593262)[0m              'save_freq': 1,
[36m(main_task pid=2593262)[0m              'test_freq': 100,
[36m(main_task pid=2593262)[0m              'total_epochs': 15,
[36m(main_task pid=2593262)[0m              'total_training_steps': 1005,
[36m(main_task pid=2593262)[0m              'val_before_train': False,
[36m(main_task pid=2593262)[0m              'val_only': False}}
[36m(main_task pid=2593262)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=2593262)[0m No module named 'vllm._version'
[36m(main_task pid=2593262)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(main_task pid=2593262)[0m original dataset len: 79168
[36m(main_task pid=2593262)[0m filter dataset len: 79168
[36m(main_task pid=2593262)[0m filtered training dataset size: 79168
[36m(main_task pid=2593262)[0m original dataset len: 3610
[36m(main_task pid=2593262)[0m filter dataset len: 3610
[36m(main_task pid=2593262)[0m filtered validation dataset size: 128
[36m(main_task pid=2593262)[0m Size of train dataloader: 618
[36m(main_task pid=2593262)[0m Size of val dataloader: 1
[36m(main_task pid=2593262)[0m Total training steps: 1005
[36m(main_task pid=2593262)[0m wandb: Tracking run with wandb version 0.24.1
[36m(main_task pid=2593262)[0m wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
[36m(main_task pid=2593262)[0m wandb: Run data is saved locally in /public_hw/share/cit_ztyu/cz/Search-R1/wandb_offline/wandb/offline-run-20260208_094356-yr87ne2q
[36m(main_task pid=2593262)[0m wandb: Detected [openai] in use.
[36m(main_task pid=2593262)[0m wandb: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[36m(main_task pid=2593262)[0m wandb: For more information, check out the docs at: https://weave-docs.wandb.ai/
[36m(pid=gcs_server)[0m [2026-02-08 09:44:02,875 E 2586630 2586630] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[33m(raylet)[0m [2026-02-08 09:44:05,891 E 2587347 2587347] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(pid=2587481)[0m [2026-02-08 09:44:07,342 E 2587481 2587817] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[2026-02-08 09:44:08,252 E 2586109 2587471] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(pid=2595223)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2595223)[0m No module named 'vllm._version'
[36m(pid=2595223)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(main_task pid=2593262)[0m [2026-02-08 09:44:10,728 E 2593262 2593443] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 92x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=2599830)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(pid=2599830)[0m No module named 'vllm._version'
[36m(pid=2599830)[0m   from vllm.version import __version__ as VLLM_VERSION
[36m(WorkerDict pid=2595223)[0m [W208 09:44:28.805641901 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[36m(WorkerDict pid=2595223)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}
[36m(WorkerDict pid=2595223)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2595223)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
[36m(bundle_reservation_check_func pid=2594884)[0m [2026-02-08 09:44:30,558 E 2594884 2595131] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=2599830)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  1.02it/s]
[36m(WorkerDict pid=2599830)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.05s/it]
[36m(WorkerDict pid=2599830)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct and are newly initialized: ['score.bias']
[36m(WorkerDict pid=2599830)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=2595223)[0m Qwen2ForTokenClassification contains 3.09B parameters
[36m(WorkerDict pid=2595223)[0m Before critic FSDP, memory allocated (GB): 0.0, memory reserved (GB): 0.0
[36m(WorkerDict pid=2595223)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at /public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct and are newly initialized: ['score.bias', 'score.weight']
[36m(WorkerDict pid=2595223)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2599830)[0m [W208 09:44:28.797723085 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[36m(WorkerDict pid=2595223)[0m After critic FSDP, memory allocated (GB): 5.748030662536621, memory reserved (GB): 8.947265625
[36m(WorkerDict pid=2595223)[0m Total steps: 1005, num_warmup_steps: 15
[36m(WorkerDict pid=2595223)[0m Critic use_remove_padding=True
[36m(WorkerDict pid=2595223)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2595223)[0m   "_name_or_path": "/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=2595223)[0m   "architectures": [
[36m(WorkerDict pid=2595223)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2595223)[0m   ],
[36m(WorkerDict pid=2595223)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2595223)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2595223)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2595223)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=2595223)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2595223)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=2595223)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2595223)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=2595223)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2595223)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=2595223)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=2595223)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2595223)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2595223)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2595223)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2595223)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2595223)[0m   "sliding_window": null,
[36m(WorkerDict pid=2595223)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2595223)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2595223)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2595223)[0m   "use_cache": true,
[36m(WorkerDict pid=2595223)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2595223)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2595223)[0m }
[36m(WorkerDict pid=2595223)[0m 
[36m(WorkerDict pid=2599830)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2599830)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2595223)[0m [2026-02-08 09:44:34,313 E 2595223 2598131] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.77s/it]
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:00<00:00,  6.93it/s]
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.85s/it]
[36m(WorkerDict pid=2595223)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=2595223)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=2595223)[0m wrap_policy: functools.partial(<function _or_policy at 0x14544bb948b0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x14544bb94790>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  8.72it/s]
[36m(WorkerDict pid=2595223)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2599830)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2595223)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=2595223)[0m   "_name_or_path": "/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct",
[36m(WorkerDict pid=2595223)[0m   "architectures": [
[36m(WorkerDict pid=2595223)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=2595223)[0m   ],
[36m(WorkerDict pid=2595223)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=2595223)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=2595223)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=2595223)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=2595223)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=2595223)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=2595223)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=2595223)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=2595223)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=2595223)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=2595223)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=2595223)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=2595223)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=2595223)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=2595223)[0m   "rope_scaling": null,
[36m(WorkerDict pid=2595223)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=2595223)[0m   "sliding_window": null,
[36m(WorkerDict pid=2595223)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=2595223)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=2595223)[0m   "transformers_version": "4.47.1",
[36m(WorkerDict pid=2595223)[0m   "use_cache": true,
[36m(WorkerDict pid=2595223)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=2595223)[0m   "vocab_size": 151936
[36m(WorkerDict pid=2595223)[0m }
[36m(WorkerDict pid=2595223)[0m 
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2595223)[0m Qwen2ForCausalLM contains 3.09B parameters
[36m(WorkerDict pid=2599830)[0m Total steps: 1005, num_warmup_steps: 15
[36m(WorkerDict pid=2599830)[0m Critic use_remove_padding=True
[36m(WorkerDict pid=2595223)[0m wrap_policy: functools.partial(<function _or_policy at 0x14544bb948b0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x14544bb94790>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2599830)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2595223)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.86s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.92s/it]
[36m(WorkerDict pid=2599830)[0m Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:02<00:02,  2.38s/it][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2599830)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  8.02it/s]
[36m(WorkerDict pid=2595223)[0m Total steps: 1005, num_warmup_steps: 286
[36m(WorkerDict pid=2595223)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2595223)[0m Before building vllm rollout, memory allocated (GB): 14.370079517364502, memory reserved (GB): 18.9296875
[36m(WorkerGroupRegisterCenter pid=2599546)[0m [2026-02-08 09:44:46,624 E 2599546 2599663] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=2595223)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=2599830)[0m Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.95s/it]
[36m(WorkerDict pid=2595223)[0m WARNING 02-08 09:44:58 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2599830)[0m wrap_policy: functools.partial(<function _or_policy at 0x14af5f0228b0>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x14af5f022790>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=2599830)[0m Total steps: 1005, num_warmup_steps: 286
[36m(WorkerDict pid=2599830)[0m Actor use_remove_padding=True
[36m(WorkerDict pid=2595223)[0m local rank 0
[36m(WorkerDict pid=2595223)[0m INFO 02-08 09:44:58 selector.py:115] Using XFormers backend.
[36m(WorkerDict pid=2595223)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
[36m(WorkerDict pid=2595223)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2599830)[0m [2026-02-08 09:44:49,216 E 2599830 2599956] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[36m(WorkerDict pid=2595223)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(WorkerDict pid=2599830)[0m NCCL version 2.20.5+cuda12.4
[36m(WorkerDict pid=2595223)[0m before init cache memory allocated: 21.643579904GB, reserved: 21.730689024GB
[36m(WorkerDict pid=2595223)[0m after init cache memory allocated: 45.658264064GB, reserved: 45.743079424GB
[36m(WorkerDict pid=2599830)[0m WARNING 02-08 09:44:58 config.py:380] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=2595223)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(WorkerDict pid=2595223)[0m After building vllm rollout, memory allocated (GB): 36.77456331253052, memory reserved (GB): 42.6015625
[36m(WorkerDict pid=2595223)[0m After building sharding manager, memory allocated (GB): 36.77456331253052, memory reserved (GB): 42.6015625
[36m(WorkerDict pid=2599830)[0m local rank 0
[36m(WorkerDict pid=2599830)[0m INFO 02-08 09:44:59 selector.py:115] Using XFormers backend.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2595223)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2595223)[0m   warnings.warn(
[36m(WorkerDict pid=2599830)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=2599830)[0m   @torch.library.impl_abstract("xformers_flash::flash_fwd")
[36m(WorkerDict pid=2599830)[0m   @torch.library.impl_abstract("xformers_flash::flash_bwd")
[36m(main_task pid=2593262)[0m epoch 0, step 1
[36m(main_task pid=2593262)[0m ACTIVE_TRAJ_NUM: [128, 112, 19, 11]
[36m(WorkerDict pid=2599830)[0m kwargs: {'n': 1, 'logprobs': 1, 'max_tokens': 2048, 'detokenize': False, 'temperature': 1.0, 'top_k': -1, 'top_p': 0.95, 'ignore_eos': False}
[36m(main_task pid=2593262)[0m --------------------------------
[36m(main_task pid=2593262)[0m Golden answers: ['Maddie Ziegler']
[36m(main_task pid=2593262)[0m Extracted answer: Maddie Ziegler
[36m(main_task pid=2593262)[0m Solution string: <|im_start|>system
[36m(main_task pid=2593262)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=2593262)[0m <|im_start|>user
[36m(main_task pid=2593262)[0m Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: who is the little girl in sia videos?
[36m(main_task pid=2593262)[0m <|im_end|>
[36m(main_task pid=2593262)[0m <|im_start|>assistant
[36m(main_task pid=2593262)[0m <think>
[36m(main_task pid=2593262)[0m To answer this question, I need to think about the specific videos where Sia (Adele Anna Sioma Adebowale) appears with young girls. Sia is known to have featured young models or song interpreters in some of her videos, but the specific "little girl" isn't immediately clear from that.
[36m(main_task pid=2593262)[0m </think>
[36m(main_task pid=2593262)[0m <search> Sia videos little girl </search>
[36m(main_task pid=2593262)[0m 
[36m(main_task pid=2593262)[0m <information>Doc 1(Title: "Maddie Ziegler") Maddie Ziegler Madison Nicole Ziegler ( ; born September 30, 2002), is an American dancer, actress and model. She first became known for appearing in Lifetime's reality show ""Dance Moms"" from 2011 (at age 8) until 2016. From 2014 to 2017, she gained wider fame for starring in a series of music videos by Sia, including ""Chandelier"" and ""Elastic Heart"", which cumulatively have attracted more than 4 billion views on YouTube. Ziegler has appeared in films, television and concerts, and has modeled on magazine covers and in advertisements for Capezio, Ralph Lauren and Target, among other brands. She was included
[36m(main_task pid=2593262)[0m Doc 2(Title: "Maddie Ziegler") Choice Awards. Maddie Ziegler Madison Nicole Ziegler ( ; born September 30, 2002), is an American dancer, actress and model. She first became known for appearing in Lifetime's reality show ""Dance Moms"" from 2011 (at age 8) until 2016. From 2014 to 2017, she gained wider fame for starring in a series of music videos by Sia, including ""Chandelier"" and ""Elastic Heart"", which cumulatively have attracted more than 4 billion views on YouTube. Ziegler has appeared in films, television and concerts, and has modeled on magazine covers and in advertisements for Capezio, Ralph Lauren and Target, among other brands. She
[36m(main_task pid=2593262)[0m Doc 3(Title: "Sia (musician)") ""Chandelier"": Record of the Year, Song of the Year, Best Pop Solo Performance and Best Music Video. For live performances of songs from ""1000 Forms of Fear"", Sia chose not to show her face, either facing away from audiences or hiding it behind oversized platinum blonde wigs. In videos for the singles ""Chandelier"", ""Elastic Heart"" and ""Big Girls Cry"", choreographed by Ryan Heffington and co-directed by Sia and Daniel Askill, and in many of the promotional live performances, child dancer Maddie Ziegler performed as a proxy for Sia in bobbed blonde wigs similar to Sia's familiar hairstyle. The three videos</information>
[36m(main_task pid=2593262)[0m 
[36m(main_task pid=2593262)[0m <answer>Maddie Ziegler</answer>
[36m(main_task pid=2593262)[0m --------------------------------
[36m(main_task pid=2593262)[0m Golden answers: ['2012']
[36m(main_task pid=2593262)[0m Extracted answer: The Retina display was first introduced for the 13-inch MacBook Pro in October 2012.
[36m(main_task pid=2593262)[0m Solution string: <|im_start|>system
[36m(main_task pid=2593262)[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
[36m(main_task pid=2593262)[0m <|im_start|>user
[36m(main_task pid=2593262)[0m Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search> and it will return the top searched results between <information> and </information>. You can search as many times as your want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer>, without detailed illustrations. For example, <answer> Beijing </answer>. Question: when did retina display come out for macbook pro?
[36m(main_task pid=2593262)[0m <|im_end|>
[36m(main_task pid=2593262)[0m <|im_start|>assistant
[36m(main_task pid=2593262)[0m <think>
[36m(main_task pid=2593262)[0m Let's think through this step-by-step. The Retina display was a significant upgrade for MacBook Pros, first introduced to enhance the display quality. I'll have to find the exact date for when this feature was first introduced.
[36m(main_task pid=2593262)[0m </think>
[36m(main_task pid=2593262)[0m <search>
[36m(main_task pid=2593262)[0m when did apple introduce retina display macbook pro
[36m(main_task pid=2593262)[0m </search>
[36m(main_task pid=2593262)[0m 
[36m(main_task pid=2593262)[0m <information>Doc 1(Title: "Retina display") display, Retina HD Display, Liquid Retina HD Display, Super Retina HD Display, or Retina 4K/5K Display: Higher resolution Retina screens are standard on the 3rd-generation MacBook Pro and new MacBook, released in 2013 and 2015, respectively. The 4th-generation MacBook Pro, released in 2016, retains the same Retina display of the previous generation. Reviews of Apple devices with Retina displays have generally been positive on technical grounds, with comments describing it as a considerable improvement on earlier screens and praising Apple for driving third-party application support for high-resolution displays more effectively than on Windows. While high-dpi displays such as IBM's T220
[36m(main_task pid=2593262)[0m Doc 2(Title: "MacBook Pro") is thinner than its predecessor and is the first to include a high-resolution Retina Display. A 13-inch variant was released in October 2012. The fourth generation MacBook Pro was announced on October 27, 2016. It replaces the function keys with an interactive, multi-touch ""Touch Bar"" and a Touch ID sensor integrated into the Power button. The original 15-inch MacBook Pro was announced on January 10, 2006 by Steve Jobs at the Macworld Conference & Expo. The 17-inch model was unveiled on April 24, 2006. The first design was largely a carryover from the PowerBook G4, but uses Intel Core CPUs
[36m(main_task pid=2593262)[0m Doc 3(Title: "MacBook Pro") also investigating similar cases across the United States. On February 20, 2015, Apple instituted the This ""will repair affected MacBook Pro systems, free of charge"". The program covered affected MacBook Pro models until December 31, 2016 or four years from original date of sale. On June 11, 2012 at the Apple Worldwide Developers Conference in San Francisco, Apple introduced the third generation MacBook Pro, marketed as the ""MacBook Pro with Retina display"" to differentiate it from the predecessor model. The new model includes Intel's third generation Core i7 processors (Ivy Bridge microarchitecture), USB 3.0, and a high-resolution 15.4"" IPS 2880√ó1800-pixel</information>
[36m(main_task pid=2593262)[0m 
[36m(main_task pid=2593262)[0m <answer>
[36m(main_task pid=2593262)[0m The Retina display was first introduced for the 13-inch MacBook Pro in October 2012.
[36m(main_task pid=2593262)[0m </answer>
[36m(WorkerDict pid=2595223)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=2595223)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
[36m(WorkerDict pid=2599830)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:689: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=2599830)[0m   warnings.warn(
[36m(WorkerDict pid=2599830)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.[32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=2599830)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined][32m [repeated 2x across cluster][0m
Error executing job with overrides: ['data.train_files=data/nq_search/train.parquet', 'data.val_files=data/nq_search/test.parquet', 'data.train_data_num=null', 'data.val_data_num=128', 'data.train_batch_size=128', 'data.val_batch_size=128', 'data.max_prompt_length=4096', 'data.max_response_length=2048', 'data.max_start_length=2048', 'data.max_obs_length=2048', 'data.shuffle_train_dataloader=True', 'algorithm.adv_estimator=gae', 'actor_rollout_ref.model.path=/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct', 'actor_rollout_ref.model.enable_gradient_checkpointing=true', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.285', 'actor_rollout_ref.actor.use_kl_loss=true', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.ppo_micro_batch_size=16', 'actor_rollout_ref.actor.fsdp_config.param_offload=false', 'actor_rollout_ref.actor.fsdp_config.grad_offload=false', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=false', 'actor_rollout_ref.rollout.log_prob_micro_batch_size=64', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.rollout.n_agent=1', 'actor_rollout_ref.rollout.free_cache_engine=False', 'actor_rollout_ref.ref.log_prob_micro_batch_size=64', 'actor_rollout_ref.ref.fsdp_config.param_offload=false', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'critic.optim.lr=1e-5', 'critic.model.use_remove_padding=True', 'critic.optim.lr_warmup_steps_ratio=0.015', 'critic.model.path=/public_hw/share/cit_ztyu/cz/models/Qwen2.5-3B-Instruct', 'critic.model.enable_gradient_checkpointing=true', 'critic.ppo_micro_batch_size=16', 'critic.model.fsdp_config.param_offload=false', 'critic.model.fsdp_config.grad_offload=false', 'critic.model.fsdp_config.optimizer_offload=false', 'algorithm.no_think_rl=false', 'trainer.logger=[wandb]', '+trainer.val_only=false', '+trainer.val_before_train=false', 'trainer.default_hdfs_dir=null', 'trainer.n_gpus_per_node=2', 'trainer.nnodes=1', 'trainer.save_freq=1', 'trainer.test_freq=100', 'trainer.project_name=Search-R1', 'trainer.experiment_name=nq-search-r1-ppo-qwen2.5-3b-instruct-em', 'trainer.total_epochs=15', 'trainer.total_training_steps=1005', 'trainer.default_local_dir=verl_checkpoints/nq-search-r1-ppo-qwen2.5-3b-instruct-em', 'max_turns=2', 'retriever.url=http://127.0.0.1:8009/retrieve', 'retriever.topk=3']
Traceback (most recent call last):
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/trainer/main_ppo.py", line 207, in <module>
    main()
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/trainer/main_ppo.py", line 114, in main
    ray.get(main_task.remote(config))
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::main_task()[39m (pid=2593262, ip=172.18.103.5)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/trainer/main_ppo.py", line 203, in main_task
    trainer.fit()
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/trainer/ppo/ray_trainer.py", line 840, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/single_controller/ray/base.py", line 42, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=2599830, ip=172.18.103.5, actor_id=641cbf42dfc00863bb9531ef01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x14b09863cee0>)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/single_controller/ray/base.py", line 399, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/single_controller/base/decorator.py", line 404, in inner
    return func(*args, **kwargs)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/workers/fsdp_workers.py", line 375, in update_actor
    metrics = self.actor.update_policy(data=data)
  File "/public_hw/share/cit_ztyu/cz/Search-R1/verl/workers/actor/dp_actor.py", line 276, in update_policy
    loss.backward()
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.21 GiB. GPU 0 has a total capacity of 79.18 GiB of which 1.69 GiB is free. Process 2578667 has 1.13 GiB memory in use. Including non-PyTorch memory, this process has 76.33 GiB memory in use. Of the allocated memory 74.08 GiB is allocated by PyTorch, and 347.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=2595223)[0m /public_hw/home/cit_zhishuliu/miniconda3/envs/searchr1/lib/python3.9/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
[36m(WorkerDict pid=2595223)[0m   with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]
===== ËÆ≠ÁªÉÁªìÊùü/‰∏≠Êñ≠ÔºåÂºÄÂßãÂÖ≥Èó≠Ê£ÄÁ¥¢ÊúçÂä° =====
Ê£ÄÁ¥¢ÊúçÂä°ËøõÁ®ã 2578667 Â∑≤ÂÖ≥Èó≠
===== Ê∏ÖÁêÜÂÆåÊàê =====
